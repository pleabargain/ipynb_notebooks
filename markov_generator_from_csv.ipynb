{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "markov generator from csv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pleabargain/ipynb_notebooks/blob/master/markov_generator_from_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2sRaBLIzBX9Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#License\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2018 Ashwin M J\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "metadata": {
        "id": "j_IWdyjPre7O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Prediction using Markov Model"
      ]
    },
    {
      "metadata": {
        "id": "moJpPYyZre7P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook makes use of Markov model for word prediction. Specifically 2nd order Markov model is deployed here for next word prediction. As an example of the Markov chain, an attempt is made to generate a new song lyrics from a bunch of Eminem song lyrics."
      ]
    },
    {
      "metadata": {
        "id": "oNUytzuVre7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preamble\n",
        "import string\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8FCVbyDre7T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Path of the text file containing the training data\n",
        "training_data_file = 'test.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z3g3GjXqre7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "pXRQxdw2re7X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "-A_7Ec44re7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_punctuation(sentence):\n",
        "    return sentence.translate(str.maketrans('','', string.punctuation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9Fsul1Ere7b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add2dict(dictionary, key, value):\n",
        "    if key not in dictionary:\n",
        "        dictionary[key] = []\n",
        "    dictionary[key].append(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01fKah9Lre7d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def list2probabilitydict(given_list):\n",
        "    probability_dict = {}\n",
        "    given_list_length = len(given_list)\n",
        "    for item in given_list:\n",
        "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
        "    for key, value in probability_dict.items():\n",
        "        probability_dict[key] = value / given_list_length\n",
        "    return probability_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMTWBb_Jre7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initial_word = {}\n",
        "second_word = {}\n",
        "transitions = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YJrmYJXbre7j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training function"
      ]
    },
    {
      "metadata": {
        "id": "CZMMk2gsre7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Trains a Markov model based on the data in training_data_file\n",
        "def train_markov_model():\n",
        "    for line in open(training_data_file):\n",
        "        tokens = remove_punctuation(line.rstrip().lower()).split()\n",
        "        tokens_length = len(tokens)\n",
        "        for i in range(tokens_length):\n",
        "            token = tokens[i]\n",
        "            if i == 0:\n",
        "                initial_word[token] = initial_word.get(token, 0) + 1\n",
        "            else:\n",
        "                prev_token = tokens[i - 1]\n",
        "                if i == tokens_length - 1:\n",
        "                    add2dict(transitions, (prev_token, token), 'END')\n",
        "                if i == 1:\n",
        "                    add2dict(second_word, prev_token, token)\n",
        "                else:\n",
        "                    prev_prev_token = tokens[i - 2]\n",
        "                    add2dict(transitions, (prev_prev_token, prev_token), token)\n",
        "    \n",
        "    # Normalize the distributions\n",
        "    initial_word_total = sum(initial_word.values())\n",
        "    for key, value in initial_word.items():\n",
        "        initial_word[key] = value / initial_word_total\n",
        "        \n",
        "    for prev_word, next_word_list in second_word.items():\n",
        "        second_word[prev_word] = list2probabilitydict(next_word_list)\n",
        "        \n",
        "    for word_pair, next_word_list in transitions.items():\n",
        "        transitions[word_pair] = list2probabilitydict(next_word_list)\n",
        "    \n",
        "    print('Training successful.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNSteH1Tre7n",
        "colab_type": "code",
        "outputId": "11f09beb-090a-4f57-a47d-8dcb3d023e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_markov_model()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training successful.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "51XZvI2pre7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://)## Testing"
      ]
    },
    {
      "metadata": {
        "id": "ktDDyn5rre7s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "axM4eF73re7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_word(dictionary):\n",
        "    p0 = np.random.random()\n",
        "    cumulative = 0\n",
        "    for key, value in dictionary.items():\n",
        "        cumulative += value\n",
        "        if p0 < cumulative:\n",
        "            return key\n",
        "    assert(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnpeMnhhre7v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test functions"
      ]
    },
    {
      "metadata": {
        "id": "mCmBWx_dre7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "number_of_sentences = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pMoqnI_Wre7z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to generate sample text\n",
        "def generate():\n",
        "    for i in range(number_of_sentences):\n",
        "        sentence = []\n",
        "        # Initial word\n",
        "        word0 = sample_word(initial_word)\n",
        "        sentence.append(word0)\n",
        "        # Second word\n",
        "        word1 = sample_word(second_word[word0])\n",
        "        sentence.append(word1)\n",
        "        # Subsequent words untill END\n",
        "        while True:\n",
        "            word2 = sample_word(transitions[(word0, word1)])\n",
        "            if word2 == 'END':\n",
        "                break\n",
        "            sentence.append(word2)\n",
        "            word0 = word1\n",
        "            word1 = word2\n",
        "        print(' '.join(sentence),\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bd-_69Were71",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing arena"
      ]
    },
    {
      "metadata": {
        "id": "eXAyMUd7re72",
        "colab_type": "code",
        "outputId": "27202234-0819-457e-f68f-e82860378da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "generate()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "theres vomit on his sweater already moms spaghetti \n",
            "\n",
            "if all its gonna cause is pain \n",
            "\n",
            "theyre asking me to rap like im on my soapbox and preach my sermon and speech \n",
            "\n",
            "stay in one spot another day of monotonys gotten me \n",
            "\n",
            "fuck can i say if life was a highway \n",
            "\n",
            "and its no movie theres no mekhi phifer this is the plot listen up you bizzels forgot slizzel does not give up \n",
            "\n",
            "you rodent im omnipotent \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qfDgje4JEutN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All in all there is some new variaton but not as much as I had anticipated.\n",
        "\n",
        "The data set might not be large enough."
      ]
    }
  ]
}